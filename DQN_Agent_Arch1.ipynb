{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cab-Driver Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing libraries\n",
    "import numpy as np\n",
    "import random\n",
    "import math\n",
    "from collections import deque\n",
    "import collections\n",
    "import pickle\n",
    "\n",
    "\n",
    "\n",
    "# for plotting graphs\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Import the environment\n",
    "from Env import CabDriver"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "# for building DQN model\n",
    "from keras import layers\n",
    "from keras import Sequential\n",
    "from keras.layers import Dense, Activation, Flatten\n",
    "from keras.optimizers import Adam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Calling env.py file as environment\n",
    "env = CabDriver()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Defining Time Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading the time matrix provided\n",
    "Time_matrix = np.load(\"TM.npy\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Tracking the state-action pairs for checking convergence\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def encoded_state(state):\n",
    "    return ('-'.join(str(e) for e in state))\n",
    "def encoded_action(action):\n",
    "    return ('-'.join(str(e) for e in action))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tracking q-values for all possible moves from position 4 at the start of the episode\n",
    "def track_states_(episodes):\n",
    "    # Sample state action pairs for tracking\n",
    "    sample_values = [('4-0-0', '1-2'), ('4-0-0', '2-1'), ('4-0-0', '1-3'), ('4-0-0', '3-1'), ('4-0-0', '1-4'), ('4-0-0', '4-1'), ('4-0-0', '1-5'), ('4-0-0', '5-1'), ('4-0-0', '2-3'), \n",
    "                       ('4-0-0', '3-2'), ('4-0-0', '2-4'), ('4-0-0', '4-2'), ('4-0-0', '2-5'), ('4-0-0', '5-2'), ('4-0-0', '3-4'), ('4-0-0', '4-3'), ('4-0-0', '3-5'), ('4-0-0', '5-3'), \n",
    "                       ('4-0-0', '4-5'), ('4-0-0', '5-4'), ('4-0-0', '0-0')]    \n",
    "    for q_value in sample_values:\n",
    "        state = q_value[0]\n",
    "        action = q_value[1]\n",
    "        States_track[state][action] = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to save q-value for tracking states-action pair\n",
    "def tracking_saved(current_state, current_action, current_q_value):\n",
    "    for state in States_track.keys():\n",
    "        if state == current_state:\n",
    "            for action in States_track[state].keys():\n",
    "                if action == current_action:\n",
    "                    States_track[state][action].append(current_q_value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Defining a function to save the Q-dictionary as a pickle file\n",
    "def save_obj(obj, name ):\n",
    "    with open(name + '.pkl', 'wb') as f:\n",
    "        pickle.dump(obj, f, pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Agent Class\n",
    "\n",
    "If you are using this framework, you need to fill the following to complete the following code block:\n",
    "1. State and Action Size\n",
    "2. Hyperparameters\n",
    "3. Create a neural-network model in function 'build_model()'\n",
    "4. Define epsilon-greedy strategy in function 'get_action()'\n",
    "5. Complete the function 'append_sample()'. This function appends the recent experience tuple <state, action, reward, new-state> to the memory\n",
    "6. Complete the 'train_model()' function with following logic:\n",
    "   - If the memory size is greater than mini-batch size, you randomly sample experiences from memory as per the mini-batch size and do the following:\n",
    "      - Initialise your input and output batch for training the model\n",
    "      - Calculate the target Q value for each sample: reward + gamma*max(Q(s'a,))\n",
    "      - Get Q(s', a) values from the last trained model\n",
    "      - Update the input batch as your encoded state and output batch as your Q-values\n",
    "      - Then fit your DQN model using the updated input and output batch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DQNAgent:\n",
    "    def __init__(self, state_size, action_size):\n",
    "        # Define size of state and action\n",
    "        self.state_size = state_size\n",
    "        self.action_size = action_size\n",
    "\n",
    "        # Write here: Specify you hyper parameters for the DQN\n",
    "        self.discount_factor = 0.98\n",
    "        self.learning_rate =  0.01      \n",
    "        self.epsilon = 1.0\n",
    "        self.epsilon_max = 1.0\n",
    "        self.epsilon_decay = -0.0008\n",
    "        self.epsilon_min = 0.00001\n",
    "        \n",
    "        self.batch_size = 32  \n",
    "        self.train_start = 50\n",
    "        # create replay memory using deque\n",
    "        self.memory = deque(maxlen=2000)\n",
    "\n",
    "       # create main model and target model\n",
    "        #print (\"Main NN Model : \")\n",
    "        self.model = self.build_model()\n",
    "        print (\"\\nTarget NN Model : \")\n",
    "        self.target_model = self.build_model()\n",
    "\n",
    "        \n",
    "            \n",
    "    # approximate Q function using Neural Network\n",
    "    def build_model(self):\n",
    "        model = Sequential()\n",
    "        # Write your code here: Add layers to your neural nets       \n",
    "        # input layer - input dimension would be state size\n",
    "        model.add(Dense(32, input_dim=self.state_size, activation='relu', kernel_initializer='he_uniform'))\n",
    "        \n",
    "        # layer - 2\n",
    "        model.add(Dense(32, activation='relu', kernel_initializer='he_uniform'))\n",
    "        \n",
    "        # layer - 3\n",
    "        model.add(Dense(32, activation='relu', kernel_initializer='he_uniform'))\n",
    "        \n",
    "        # layer - 4\n",
    "        model.add(Dense(32, activation='relu', kernel_initializer='he_uniform'))\n",
    "\n",
    "        # the output layer: output size is total number of possible actions     \n",
    "        model.add(Dense(self.action_size, activation='relu', kernel_initializer='he_uniform'))\n",
    "        model.compile(loss='mse',optimizer=Adam(lr=self.learning_rate))\n",
    "        model.summary\n",
    "        return model\n",
    "    def update_target_model(self):\n",
    "        # update the target Q-value network to current Q-value network after training for a episode. \n",
    "        # this means that weights an biases of target Q-value network will become same as current Q-value network.\n",
    "        self.target_model.set_weights(self.model.get_weights())\n",
    "\n",
    "\n",
    "    def get_action(self, state, possible_actions):      \n",
    "        if np.random.rand() <= self.epsilon:\n",
    "            # explore: choose a random action from all possible actions\n",
    "            # possible actions list would be given by environment\n",
    "            choice = random.choice(possible_actions)\n",
    "        else:\n",
    "            # choose the action with the highest q(s, a)\n",
    "            q_values = self.model.predict(state)\n",
    "            possible_action_q_values = list(q_values[0][index] for index in possible_actions)\n",
    "            max_q_value = np.max(possible_action_q_values)\n",
    "            choice = np.where(q_values[0] == max_q_value)\n",
    "            choice = choice[0][0]\n",
    "            \n",
    "        return choice\n",
    "        \n",
    "    def get_q_values(self, state):\n",
    "        return self.model.predict(state)[0]\n",
    "    \n",
    "    def append_sample(self, state, action, reward, next_state, done):\n",
    "        # append the tuple (s, a, r, s', done) to memory (replay buffer) after every action\n",
    "        self.memory.append((state, action, reward, next_state, done))\n",
    "    \n",
    "        # decay in Îµ after we generate each sample from the environment\n",
    "        if self.epsilon > self.epsilon_min:\n",
    "            self.epsilon *= self.epsilon_decay    \n",
    "    \n",
    "    def train_model(self):\n",
    "        # don't start the model training until memory queue is of certain size\n",
    "        if len(self.memory) < self.train_start:\n",
    "            return\n",
    "        \n",
    "        # sample batch from the memory\n",
    "        batch_size = min(self.batch_size, len(self.memory))\n",
    "        # pick samples randomly from replay memory (with batch_size) and train the network\n",
    "        mini_batch = random.sample(self.memory, self.batch_size)\n",
    "\n",
    "        # initialise your input and output batch for training the model\n",
    "        update_input = np.zeros((self.batch_size, self.state_size))\n",
    "        update_target = np.zeros((self.batch_size, self.state_size)) \n",
    "        actions, rewards, terminal_states = [], [], []\n",
    "\n",
    "        for i in range(self.batch_size):\n",
    "\n",
    "            update_input[i] = mini_batch[i][0]\n",
    "            actions.append(mini_batch[i][1])\n",
    "            rewards.append(mini_batch[i][2])\n",
    "            update_target[i] = mini_batch[i][3]\n",
    "            terminal_states.append(mini_batch[i][4])\n",
    "\n",
    "        # predict the target from earlier model\n",
    "        target = self.model.predict(update_input)\n",
    "\n",
    "        # get the target for the Q-network\n",
    "        target_qval = self.target_model.predict(update_target)  \n",
    "\n",
    "        for i in range(self.batch_size):\n",
    "            if terminal_states[i]:\n",
    "                target[i][actions[i]] = rewards[i]\n",
    "            else: # non-terminal state\n",
    "                target[i][actions[i]] = rewards[i] + self.discount_factor * np.max(target_qval[i])\n",
    "\n",
    "        # fit your model and track the loss values\n",
    "        self.model.fit(update_input, target, batch_size=self.batch_size, epochs=1, verbose=0)\n",
    "    \n",
    "    def save(self, name):\n",
    "        self.model.save_weights(name)\n",
    "        \n",
    "    def get_model_weights(self):\n",
    "        return self.model.get_weights()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "Episodes = 10000"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DQN block"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-10-18 22:43:23.640627: I tensorflow/core/platform/cpu_feature_guard.cc:145] This TensorFlow binary is optimized with Intel(R) MKL-DNN to use the following CPU instructions in performance critical operations:  SSE4.1 SSE4.2\n",
      "To enable them in non-MKL-DNN operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2021-10-18 22:43:23.641021: I tensorflow/core/common_runtime/process_util.cc:115] Creating new thread pool with default inter op setting: 8. Tune using inter_op_parallelism_threads for best performance.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Target NN Model : \n",
      "Episode: 100 rewards: 1067 memory length: 2000 epsilon: 0.9238458963884149\n",
      "Episode: 200 rewards: 849 memory length: 2000 epsilon: 0.85281724849836\n",
      "Episode: 300 rewards: 1088 memory length: 2000 epsilon: 0.7872495425693098\n",
      "Episode: 400 rewards: 1216 memory length: 2000 epsilon: 0.7267229214311316\n",
      "Episode: 500 rewards: 1130 memory length: 2000 epsilon: 0.6708498080669284\n",
      "Episode: 600 rewards: 1151 memory length: 2000 epsilon: 0.6192724237969188\n",
      "Episode: 700 rewards: 473 memory length: 2000 epsilon: 0.571660497273408\n",
      "Episode: 800 rewards: 901 memory length: 2000 epsilon: 0.5277091496165958\n",
      "Episode: 900 rewards: 966 memory length: 2000 epsilon: 0.4871369421488704\n",
      "Episode: 1000 rewards: 1085 memory length: 2000 epsilon: 0.4496840742264232\n",
      "Episode: 1100 rewards: 1130 memory length: 2000 epsilon: 0.41511071962815244\n",
      "Episode: 1200 rewards: 1261 memory length: 2000 epsilon: 0.3831954908490673\n",
      "Episode: 1300 rewards: 1364 memory length: 2000 epsilon: 0.35373402146442445\n",
      "Episode: 1400 rewards: 1125 memory length: 2000 epsilon: 0.32653765748689134\n",
      "Episode: 1500 rewards: 1121 memory length: 2000 epsilon: 0.30143224933694984\n",
      "Episode: 1600 rewards: 1256 memory length: 2000 epsilon: 0.2782570366910305\n",
      "Episode: 1700 rewards: 1025 memory length: 2000 epsilon: 0.2568636190665962\n",
      "Episode: 1800 rewards: 977 memory length: 2000 epsilon: 0.23711500555240486\n",
      "Episode: 1900 rewards: 927 memory length: 2000 epsilon: 0.2188847375989828\n",
      "Episode: 2000 rewards: 1088 memory length: 2000 epsilon: 0.20205607925217048\n",
      "Episode: 2100 rewards: 1385 memory length: 2000 epsilon: 0.18652126964447216\n",
      "Episode: 2200 rewards: 1079 memory length: 2000 epsilon: 0.17218083295760164\n",
      "Episode: 2300 rewards: 975 memory length: 2000 epsilon: 0.15894294143762888\n",
      "Episode: 2400 rewards: 1232 memory length: 2000 epsilon: 0.146722827383849\n",
      "Episode: 2500 rewards: 1144 memory length: 2000 epsilon: 0.1354422403460957\n",
      "Episode: 2600 rewards: 1364 memory length: 2000 epsilon: 0.12502894605470843\n",
      "Episode: 2700 rewards: 1234 memory length: 2000 epsilon: 0.11541626387459425\n",
      "Episode: 2800 rewards: 886 memory length: 2000 epsilon: 0.10654263982151128\n",
      "Episode: 2900 rewards: 1117 memory length: 2000 epsilon: 0.09835125240642083\n",
      "Episode: 3000 rewards: 1364 memory length: 2000 epsilon: 0.090789648783965\n",
      "Episode: 3100 rewards: 1391 memory length: 2000 epsilon: 0.08380940887517964\n",
      "Episode: 3200 rewards: 1234 memory length: 2000 epsilon: 0.07736583531367952\n",
      "Episode: 3300 rewards: 822 memory length: 2000 epsilon: 0.071417667229914\n",
      "Episode: 3400 rewards: 1224 memory length: 2000 epsilon: 0.06592681604073476\n",
      "Episode: 3500 rewards: 978 memory length: 2000 epsilon: 0.060858121552426925\n",
      "Episode: 3600 rewards: 1261 memory length: 2000 epsilon: 0.056179126815430136\n",
      "Episode: 3700 rewards: 1044 memory length: 2000 epsilon: 0.05185987028905134\n",
      "Episode: 3800 rewards: 1209 memory length: 2000 epsilon: 0.047872693985313915\n",
      "Episode: 3900 rewards: 1313 memory length: 2000 epsilon: 0.04419206636340846\n",
      "Episode: 4000 rewards: 1040 memory length: 2000 epsilon: 0.04079441884066535\n",
      "Episode: 4100 rewards: 808 memory length: 2000 epsilon: 0.03765799487316114\n",
      "Episode: 4200 rewards: 833 memory length: 2000 epsilon: 0.03476271063955918\n",
      "Episode: 4300 rewards: 937 memory length: 2000 epsilon: 0.0320900264360857\n",
      "Episode: 4400 rewards: 1224 memory length: 2000 epsilon: 0.029622827959129988\n",
      "Episode: 4500 rewards: 1153 memory length: 2000 epsilon: 0.027345316715271954\n",
      "Episode: 4600 rewards: 998 memory length: 2000 epsilon: 0.025242908856987246\n",
      "Episode: 4700 rewards: 1291 memory length: 2000 epsilon: 0.02330214179623291\n",
      "Episode: 4800 rewards: 1184 memory length: 2000 epsilon: 0.021510587997921844\n",
      "Episode: 4900 rewards: 1283 memory length: 2000 epsilon: 0.019856775401269837\n",
      "Episode: 5000 rewards: 856 memory length: 2000 epsilon: 0.018330113959440234\n",
      "Episode: 5100 rewards: 1032 memory length: 2000 epsilon: 0.01692082782708914\n",
      "Episode: 5200 rewards: 877 memory length: 2000 epsilon: 0.015619892761579842\n",
      "Episode: 5300 rewards: 945 memory length: 2000 epsilon: 0.01441897833702064\n",
      "Episode: 5400 rewards: 1088 memory length: 2000 epsilon: 0.013310394601098544\n",
      "Episode: 5500 rewards: 977 memory length: 2000 epsilon: 0.012287042833130488\n",
      "Episode: 5600 rewards: 1214 memory length: 2000 epsilon: 0.011342370088015512\n",
      "Episode: 5700 rewards: 815 memory length: 2000 epsilon: 0.010470327235013944\n",
      "Episode: 5800 rewards: 595 memory length: 2000 epsilon: 0.009665330222658558\n",
      "Episode: 5900 rewards: 1143 memory length: 2000 epsilon: 0.008922224321760904\n",
      "Episode: 6000 rewards: 894 memory length: 2000 epsilon: 0.008236251117545905\n",
      "Episode: 6100 rewards: 575 memory length: 2000 epsilon: 0.0076030180395518215\n",
      "Episode: 6200 rewards: 1154 memory length: 2000 epsilon: 0.007018470234182759\n",
      "Episode: 6300 rewards: 1229 memory length: 2000 epsilon: 0.006478864599802144\n",
      "Episode: 6400 rewards: 791 memory length: 2000 epsilon: 0.005980745818103069\n",
      "Episode: 6500 rewards: 801 memory length: 2000 epsilon: 0.005520924228274455\n",
      "Episode: 6600 rewards: 887 memory length: 2000 epsilon: 0.005096455402282171\n",
      "Episode: 6700 rewards: 818 memory length: 2000 epsilon: 0.00470462129047715\n",
      "Episode: 6800 rewards: 1075 memory length: 2000 epsilon: 0.004342912816798045\n",
      "Episode: 6900 rewards: 1061 memory length: 2000 epsilon: 0.0040090138121183045\n",
      "Episode: 7000 rewards: 1044 memory length: 2000 epsilon: 0.0037007861828562074\n",
      "Episode: 7100 rewards: 620 memory length: 2000 epsilon: 0.0034162562198763666\n",
      "Episode: 7200 rewards: 858 memory length: 2000 epsilon: 0.0031536019600128936\n",
      "Episode: 7300 rewards: 864 memory length: 2000 epsilon: 0.002911141519284835\n",
      "Episode: 7400 rewards: 1044 memory length: 2000 epsilon: 0.0026873223230966573\n",
      "Episode: 7500 rewards: 1054 memory length: 2000 epsilon: 0.0024807111644602324\n",
      "Episode: 7600 rewards: 1101 memory length: 2000 epsilon: 0.002289985026577066\n",
      "Episode: 7700 rewards: 667 memory length: 2000 epsilon: 0.002113922611013924\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/y1/tkc6fk0d24q0fy303204nyph0000gn/T/ipykernel_79959/2272439594.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     46\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     47\u001b[0m         \u001b[0;31m# train the model by calling function agent.train_model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 48\u001b[0;31m         \u001b[0magent\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     49\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     50\u001b[0m         \u001b[0;31m# keep track of rewards\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/var/folders/y1/tkc6fk0d24q0fy303204nyph0000gn/T/ipykernel_79959/3864725938.py\u001b[0m in \u001b[0;36mtrain_model\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    115\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    116\u001b[0m         \u001b[0;31m# fit your model and track the loss values\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 117\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mupdate_input\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    118\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    119\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0msave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/envs/tensorflow/lib/python3.7/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_freq, max_queue_size, workers, use_multiprocessing, **kwargs)\u001b[0m\n\u001b[1;32m   1152\u001b[0m             \u001b[0msample_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msample_weight\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1153\u001b[0m             \u001b[0mclass_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mclass_weight\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1154\u001b[0;31m             batch_size=batch_size)\n\u001b[0m\u001b[1;32m   1155\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1156\u001b[0m         \u001b[0;31m# Prepare validation data.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/envs/tensorflow/lib/python3.7/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36m_standardize_user_data\u001b[0;34m(self, x, y, sample_weight, class_weight, check_array_lengths, batch_size)\u001b[0m\n\u001b[1;32m    635\u001b[0m             \u001b[0;31m# Check that all arrays have the same length.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    636\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mcheck_array_lengths\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 637\u001b[0;31m                 \u001b[0mtraining_utils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcheck_array_length_consistency\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msample_weights\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    638\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_is_graph_network\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    639\u001b[0m                 \u001b[0;31m# Additional checks to avoid users mistakenly\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/envs/tensorflow/lib/python3.7/site-packages/keras/engine/training_utils.py\u001b[0m in \u001b[0;36mcheck_array_length_consistency\u001b[0;34m(inputs, targets, weights)\u001b[0m\n\u001b[1;32m    238\u001b[0m                          \u001b[0;34m'the same number of samples. Got array shapes: '\u001b[0m \u001b[0;34m+\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    239\u001b[0m                          str([y.shape for y in targets]))\n\u001b[0;32m--> 240\u001b[0;31m     \u001b[0;32mif\u001b[0m \u001b[0mset_x\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mset_y\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mset_x\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mset_y\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    241\u001b[0m         raise ValueError('Input arrays should have '\n\u001b[1;32m    242\u001b[0m                          \u001b[0;34m'the same number of samples as target arrays. '\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import pylab\n",
    "tot_rewards, episodes = [], []\n",
    "state = env.state_init\n",
    "state_encoded = env.state_encod_arch1(state)\n",
    "state_size = len(state_encoded)\n",
    "action_size = len(env.action_space)\n",
    "agent = DQNAgent(state_size, action_size)\n",
    "States_track = collections.defaultdict(dict)\n",
    "track_states_(Episodes)\n",
    "\n",
    "for episode in range(Episodes):\n",
    "    # Call all the initialised variables of the environment\n",
    "    terminal_state = False\n",
    "    rewards = 0\n",
    "    reward = 0\n",
    "    total_days = 1\n",
    "    prev_day = 0\n",
    "    curr_day = 0\n",
    "    env.reset()\n",
    "    \n",
    "\n",
    "    #Call the DQN agent\n",
    "    action_space, state_space, state = env.reset()\n",
    "    track_state = state\n",
    "    \n",
    "    while not terminal_state:\n",
    "        \n",
    "        state_encoded = env.state_encod_arch1(state)\n",
    "        state_encoded = np.reshape(state_encoded, [1, state_size])\n",
    "        \n",
    "        # get possible list of actions from the environment\n",
    "        possible_actions, action_list = env.requests(state)\n",
    "        \n",
    "        # pick epsilon-greedy action from possible actions for the current state\n",
    "        # possible actions is given by the environment\n",
    "        action = agent.get_action(state_encoded, possible_actions)\n",
    "        \n",
    "        # evaluate your reward and next state\n",
    "        reward = env.reward_func(state, env.action_space[action], Time_matrix)\n",
    "        next_state = env.next_state_func(state, env.action_space[action], Time_matrix)\n",
    "        next_state_encoded = env.state_encod_arch1(next_state)\n",
    "        next_state_encoded = np.reshape(next_state_encoded, [1, state_size])\n",
    "        \n",
    "        # append the experience to the memory\n",
    "        agent.append_sample(state_encoded, action, reward, next_state_encoded, terminal_state)\n",
    "        \n",
    "        # train the model by calling function agent.train_model\n",
    "        agent.train_model()\n",
    "        \n",
    "        # keep track of rewards\n",
    "        rewards += reward\n",
    "        prev_day = state[2]\n",
    "        state = next_state\n",
    "        curr_day = state[2]\n",
    "        if prev_day != curr_day:\n",
    "            total_days = total_days + 1\n",
    "            \n",
    "        if total_days > 30:\n",
    "            terminal_state = True\n",
    "            \n",
    "        # once you reach end of 30 days, target model is updated, tot_rewards and episode saved\n",
    "        if terminal_state == True:\n",
    "            agent.update_target_model()\n",
    "            tot_rewards.append(rewards)\n",
    "            episodes.append(episode)  \n",
    "    \n",
    "    \n",
    "    # this way is much easier to track because other states would be difficult to track as it is heavily dependant on the location and action taken\n",
    "    track_state_encode = encoded_state(track_state)\n",
    "    track_state_encoded = env.state_encod_arch1(track_state)\n",
    "    track_state_encoded = np.reshape(track_state_encoded, [1, state_size])\n",
    "    track_q_values = agent.get_q_values(track_state_encoded)\n",
    "    \n",
    "    for index in range(len(env.action_space)):\n",
    "        track_action_encode = encoded_action(env.action_space[index])\n",
    "        tracking_saved(track_state_encode, track_action_encode, track_q_values[index])\n",
    "        \n",
    "    # epsilon decay\n",
    "    agent.epsilon = (1 - 0.00001) * np.exp(agent.epsilon_decay * episode)\n",
    "    \n",
    "    #save the graph and pickle file)\n",
    "    if (episode + 1) % 100 == 0:\n",
    "        print(\"Episode:\", episode + 1, \"rewards:\", rewards, \"epsilon:\", agent.epsilon)\n",
    "        pylab.plot(episodes, tot_rewards)\n",
    "        # this will create pickle file\n",
    "        save_obj(agent.get_model_weights(), \"pickle_file\" + str(rewards))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tracking Convergence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convergence with tot_reward\n",
    "plt.plot(episodes, tot_reward)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# State-action q-value graph\n",
    "values = States_track['4-0-0']['4-3']\n",
    "plt.plot(values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# State-action q-value graph\n",
    "values = States_track['4-0-0']['4-2']\n",
    "plt.plot(values)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Epsilon-decay sample function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "Try building a similar epsilon-decay function for your model.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "time = np.arange(0,10000)\n",
    "epsilon = []\n",
    "for i in range(0,10000):\n",
    "    epsilon.append(0 + (1 - 0) * np.exp(-0.0008*i))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(time, epsilon)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
